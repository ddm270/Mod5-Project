{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from requests import get\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.display import clear_output\n",
    "from warnings import warn\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from time import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define empty list\n",
    "full_url = []\n",
    "\n",
    "# open file if already created and read the content in a list\n",
    "with open('url_list.txt', 'r') as filehandle:\n",
    "    full_url = [current_url.rstrip() for current_url in filehandle.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Proxies= [\n",
    "'12.139.101.100:80',\n",
    "'166.98.140.51:3128',\n",
    "'3.211.17.212:80',\n",
    "'167.71.201.85:8080',\n",
    "'157.245.62.184:3000',\n",
    "'52.179.231.206:80',\n",
    "'52.179.18.244:8080',\n",
    "'140.238.15.65:3128',\n",
    "'104.255.170.175:5836',\n",
    "'137.220.34.35:8080',\n",
    "'104.45.188.43:3128',\n",
    "'192.248.157.99:8080',\n",
    "'167.172.238.206:8080',\n",
    "'149.28.109.33:8080',\n",
    "'157.245.62.184:3000',\n",
    "'104.215.73.3:8080',\n",
    "'45.77.215.242:8080',\n",
    "'149.28.14.25:8080',\n",
    "'52.179.18.244:8080',\n",
    "'52.179.231.206:80',\n",
    "'198.255.114.82:3128',\n",
    "'64.227.25.67:8080',\n",
    "'12.139.101.96:80',\n",
    "'12.139.101.98:80',\n",
    "'66.42.67.39:8080',\n",
    "'18.230.159.76:8080',\n",
    "'34.195.158.33:8080',\n",
    "'167.71.201.85:8080',\n",
    "'12.139.101.100:80',\n",
    "'66.42.70.72:8080',\n",
    "'12.139.101.100:80',\n",
    "'166.98.140.51:3128',\n",
    "'3.211.17.212:80',\n",
    "'167.71.201.85:8080',\n",
    "'157.245.62.184:3000',\n",
    "'52.179.231.206:80',\n",
    "'52.179.18.244:8080',\n",
    "'140.238.15.65:3128',\n",
    "'104.255.170.175:5836',\n",
    "'137.220.34.35:8080',\n",
    "'104.45.188.43:3128',\n",
    "'192.248.157.99:8080',\n",
    "'167.172.238.206:8080',\n",
    "'149.28.109.33:8080',\n",
    "'157.245.62.184:3000',\n",
    "'104.215.73.3:8080',\n",
    "'45.77.215.242:8080',\n",
    "'149.28.14.25:8080',\n",
    "'52.179.18.244:8080',\n",
    "'52.179.231.206:80',\n",
    "'198.255.114.82:3128',\n",
    "'64.227.25.67:8080',\n",
    "'12.139.101.96:80',\n",
    "'12.139.101.98:80',\n",
    "'66.42.67.39:8080',\n",
    "'18.230.159.76:8080',\n",
    "'34.195.158.33:8080',\n",
    "'167.71.201.85:8080',\n",
    "'12.139.101.100:80',\n",
    "'66.42.70.72:8080'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent_list = [\n",
    "   #Chrome\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 5.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n",
    "    #Firefox\n",
    "    'Mozilla/4.0 (compatible; MSIE 9.0; Windows NT 6.1)',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (Windows NT 6.2; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0; Trident/5.0)',\n",
    "    'Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)',\n",
    "    'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0)',\n",
    "    'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no URL list created yet, use the code below to scrape \"Music by Genre\" pages on metacritic.com for each genre to create list of all album URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "requests_num = 0\n",
    "Headers= {'user-agent': None}\n",
    "init_url = 'https://www.metacritic.com/browse/albums/genre/date/'\n",
    "genres = ['alt-country','alternative', 'blues', 'comedy', 'country', 'dance', 'electronic', 'experimental', \n",
    "         'folk', 'house', 'indie', 'jazz', 'latin', 'metal', 'pop', 'psychedelic', 'punk', 'rap', 'rb', 'reggae',\n",
    "         'rock', 'singer-songwriter', 'soul', 'soundtrack', 'techno', 'vocal', 'world']\n",
    "\n",
    "#initialize the number of pages per genre loop. Max number of pages is the in the rock genre with 40. \n",
    "#But if a genre has less pages than that, the loop will break and go to the next genre.\n",
    "pages = [str(i) for i in range(0,45)]\n",
    "\n",
    "hrefs = []\n",
    "url_list= []\n",
    "for genre in tqdm(genres):\n",
    "    url = init_url + genre\n",
    "    Headers['user-agent']= random.choice(user_agent_list)\n",
    "\n",
    "    sleep(10)\n",
    "    for page in tqdm(pages):\n",
    "    \n",
    "    #make a get request\n",
    "        albums = get(url + '?page=' + page, headers = Headers, proxies = Proxy)\n",
    "    \n",
    "    #pause the loop for 8-20 seconds\n",
    "        sleep(randint(8,20))\n",
    "    \n",
    "    #monitor the requests\n",
    "        requests_num += 1\n",
    "        elapsed_time = time() - start_time\n",
    "        print('Request:{}; Frequency: {} requests/s'.format(requests, requests/elapsed_time))\n",
    "        clear_output(wait = True)\n",
    "    \n",
    "    #parse the  album response content into the beautiful soup object\n",
    "        album_soup = BeautifulSoup(albums.text, 'html.parser')\n",
    "        if album_soup.find(\"a\", class_='title') == None:\n",
    "            break\n",
    "        for a_elm in album_soup.find_all(\"a\", class_='title'):\n",
    "            hrefs.append((a_elm.attrs[\"href\"]))\n",
    "        sleep(10)\n",
    "for album in hrefs:\n",
    "    url_list.append('https://www.metacritic.com'+album)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hrefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_url = list(dict.fromkeys(url_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9860"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f7(seq):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in seq if not (x in seen or seen_add(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_url = f7(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabs = ['/user-reviews', '/details']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabs1 = ['/critic-reviews', '/details']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapes info from \"User Reviews\" tab of album page\n",
    "\n",
    "def user_rev(Headers, url, results, album_soup):\n",
    "    review_dict = {'name':[], 'date':[], 'rating':[], 'review':[]}\n",
    "    for user in album_soup.find_all('div', class_='review_content'): \n",
    "        if user.find('div', class_='name') == None:\n",
    "            break \n",
    "        review_dict['name'].append(user.find('div', class_='name').text)\n",
    "        review_dict['date'].append(user.find('div', class_='date').text)\n",
    "        review_dict['rating'].append(user.find('div', class_='review_grade').find_all('div')[0].text)\n",
    "        if user.find('span', class_='blurb blurb_expanded'): \n",
    "            review_dict['review'].append(user.find('span', class_='blurb blurb_expanded').text)\n",
    "        else: \n",
    "            review_dict['review'].append(user.find('div', class_='review_body').text)\n",
    "    return review_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapes info from \"Critic Reviews\" tab of album page\n",
    "\n",
    "def crit_rev(headers, url, results, album_soup):\n",
    "    review_dict2 = {'name':[], 'date':[], 'rating':[], 'review':[]}\n",
    "    for crit in album_soup.find_all('div', class_='review_content'): \n",
    "        if crit.find('div', class_='source') == None: \n",
    "            break \n",
    "        review_dict2['name'].append(crit.find('div', class_='source').text)\n",
    "        review_dict2['date'].append(crit.find('div', class_='date').text)\n",
    "        review_dict2['rating'].append(crit.find('div', class_='review_grade').find_all('div')[0].text)\n",
    "        review_dict2['review'].append(crit.find('div', class_='review_body').text)\n",
    "    return review_dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapes info from \"Details & Credits\" tab of album page\n",
    "def details(headers, url, results, album_soup):\n",
    "    deet_dict = {'title':[], 'artist':[], 'label':[], 'release_date':[], 'metascore':[], 'user_score':[], \n",
    "                     'genre':[], 'summary':[]}\n",
    "    for deet in album_soup.find_all('div', class_='col main_col'): \n",
    "        if deet.find('div', class_='product_title') == None:\n",
    "            break\n",
    "        deet_dict['title'].append(deet.find('div', class_='product_title').text)\n",
    "        deet_dict['artist'].append(deet.find('span', class_='band_name').text)\n",
    "        deet_dict['metascore'].append(deet.find_all('a', class_='metascore_anchor')[0].text)\n",
    "        deet_dict['user_score'].append(deet.find_all('a', class_='metascore_anchor')[1].text)\n",
    "        if deet.find('div', class_='new_details') == None:\n",
    "            deet_dict['label'].append(None)\n",
    "        else: \n",
    "            for d in deet.find_all('div', class_='new_details'):\n",
    "                deet_dict['label'].append(d.find('span', class_='data').text)\n",
    "        if deet.find('li', class_='summary_detail release') == None:\n",
    "            deet_dict['release_date'].append(None)\n",
    "        else:\n",
    "            for d in deet.find_all('li', class_='summary_detail release'):\n",
    "                deet_dict['release_date'].append(d.find('span', class_='data').text)\n",
    "        if deet.find('div', class_='genres') == None:\n",
    "            deet_dict['genre'].append(None)\n",
    "        else:\n",
    "            for d in deet.find_all('div', class_='genres'):\n",
    "                deet_dict['genre'].append(d.find('span', class_='data').text)\n",
    "        if deet.find('div', class_='summary_detail product_summary') == None:\n",
    "            deet_dict['summary'].append(None)\n",
    "        else:\n",
    "            for d in deet.find_all('div', class_='summary_detail product_summary'):\n",
    "                deet_dict['summary'].append(d.find('span', class_='data').text)\n",
    "    return deet_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use for data frame of user and critic reviews\n",
    "\n",
    "def album_info(init_url, tabs):\n",
    "    user_list = []\n",
    "    crit_list = []\n",
    "    deet_list = []\n",
    "    for tab in tabs:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36'}\n",
    "        url = init_url + tab\n",
    "        results = requests.get(url, headers=headers)\n",
    "        album_soup= BeautifulSoup(results.text, 'html.parser')\n",
    "        if tab == '/user-reviews':\n",
    "            user_list.append(user_rev(headers, url, results, album_soup))\n",
    "            \n",
    "        elif tab == '/critic-reviews':\n",
    "            crit_list.append(crit_rev(headers, url, results, album_soup))\n",
    "        elif tab == '/details':\n",
    "            deet_list.append(details(headers, url, results, album_soup))\n",
    "    dict1 = {k:v for element in user_list for k,v in element.items()}\n",
    "    dict2 = {k:v for element in crit_list for k,v in element.items()}\n",
    "    dict3 = {k:v for element in deet_list for k,v in element.items()}\n",
    "    user = pd.DataFrame(dict1)\n",
    "    crit = pd.DataFrame(dict2)\n",
    "    deet = pd.DataFrame(dict3)\n",
    "    user_deet = deet.loc[deet.index.repeat(len(user))].reset_index(drop=True)\n",
    "    dfu = user_deet.merge(user, how = 'left', left_index = True, right_index = True)\n",
    "    crit_deet = deet.loc[deet.index.repeat(len(crit))].reset_index(drop=True)\n",
    "    dfc = crit_deet.merge(crit, how = 'left', left_index = True, right_index = True)\n",
    "    return dfu, dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use for data frame of only user reviews\n",
    "\n",
    "def album_user(init_url, tabs):\n",
    "    user_list = []\n",
    "    deet_list = []\n",
    "    for tab in tabs:\n",
    "        Headers= {'user-agent': None}\n",
    "        Headers['user-agent']= random.choice(user_agent_list)\n",
    "        Proxy = {'proxies': None}\n",
    "        Proxy['proxies'] = random.choice(Proxies)\n",
    "        url = init_url + tab\n",
    "        results = requests.get(url, headers = Headers, proxies = Proxy)\n",
    "        album_soup= BeautifulSoup(results.text, 'html.parser')\n",
    "        if tab == '/user-reviews':\n",
    "            user_list.append(user_rev(Headers, url, results, album_soup))\n",
    "        elif tab == '/details':\n",
    "            deet_list.append(details(Headers, url, results, album_soup))\n",
    "    dict1 = {k:v for element in user_list for k,v in element.items()}\n",
    "    dict2 = {k:v for element in deet_list for k,v in element.items()}\n",
    "    user = pd.DataFrame(dict1)\n",
    "    deet = pd.DataFrame(dict2)\n",
    "    user_deet = deet.loc[deet.index.repeat(len(user))].reset_index(drop=True)\n",
    "    df = user_deet.merge(user, how = 'left', left_index = True, right_index = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use for data frame of only critic reviews\n",
    "def album_critic(init_url, tabs):\n",
    "    crit_list = []\n",
    "    deet_list = []\n",
    "    for tab in tabs:\n",
    "        Headers= {'user-agent': None}\n",
    "        Headers['user-agent']= random.choice(user_agent_list)\n",
    "        Proxy = {'proxies': None}\n",
    "        Proxy['proxies'] = random.choice(Proxies)\n",
    "        url = init_url + tab\n",
    "        results = requests.get(url, headers = Headers, proxies = Proxy)\n",
    "        album_soup= BeautifulSoup(results.text, 'html.parser')\n",
    "        if tab == '/critic-reviews':\n",
    "            crit_list.append(crit_rev(Headers, url, results, album_soup))\n",
    "        elif tab == '/details':\n",
    "            deet_list.append(details(Headers, url, results, album_soup))\n",
    "    dict1 = {k:v for element in crit_list for k,v in element.items()}\n",
    "    dict2 = {k:v for element in deet_list for k,v in element.items()}\n",
    "    crit = pd.DataFrame(dict1)\n",
    "    deet = pd.DataFrame(dict2)\n",
    "    crit_deet = deet.loc[deet.index.repeat(len(crit))].reset_index(drop=True)\n",
    "    df = crit_deet.merge(crit, how = 'left', left_index = True, right_index = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_url = []\n",
    "with open('url_list.txt', 'w') as filehandle:\n",
    "    filehandle.writelines(\"%s\\n\" % url for url in full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for url in tqdm(full_url):\n",
    "    user = album_critic(url, tabs1)\n",
    "    df = df.append(user)\n",
    "    sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
